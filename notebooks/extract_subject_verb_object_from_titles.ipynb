{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SVO triples from GOV.UK\n",
    "\n",
    "This notebook explores extracting subject verb object triples from titles on GOV.UK content\n",
    "The idea is that we'll have triples showing how the entities relate to one another\n",
    "For example in the sentence \"Apply for a passport\" the verb is apply and the object is passport, there is no subject. It's worth noting that subjects are quite rare in titles, presumably because usually it is implicitly the reader. Further work could explore assuming this except for places where it isn't. I think this ought to work but I reckon there will be edge cases where this is wrong\n",
    "\n",
    "Anyway, the cool thing about SVO triples is that we can use them to understand what you can do to various entities. It's worth noting that using the headline title of the page is the most easy/reliable way to do this - body text is more comprehensive but there are all kinds of edge cases where the verb gets negated or there's a condition or there are cases like \"Universal Credit is paid once a month\" where extracting \"paid\" is technically correct but it's not something a user is able to do...its DWP that does the paying so we ned to understand more about how to know which party in a transaction does what and whether it's government (or some other non user entity) that is the subject or not...food for thought\n",
    "\n",
    "I've also included work on FrameNet which has massive potential but is a bit more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk import Tree\n",
    "import spacy\n",
    "import json\n",
    "from py2neo import Graph\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content_dir_path = \"/Users/oscarwyatt/govuk/govuk-knowledge-extractor/govuk-production-mirror-replica\"\n",
    "preprocessed_content_store_path = \"/Users/oscarwyatt/govuk/govuk-knowledge-graph/data/preprocessed_content_store_070920.csv.gz\"\n",
    "\n",
    "all_content_items = pd.read_csv(preprocessed_content_store_path, sep=\"\\t\", compression=\"gzip\",\n",
    "                                         low_memory=False)\n",
    "\n",
    "print(\"Finished reading from the preprocessed content store!\")\n",
    "\n",
    "mainstream_content = all_content_items[all_content_items['publishing_app'] == 'publisher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOV:\n",
    "    def __init__(self):\n",
    "        self.subject = None\n",
    "        self.object = None\n",
    "        self.verb = None\n",
    "        \n",
    "    def cypher_subject(self):\n",
    "        return self._cypher_safe(self.subject)\n",
    "\n",
    "    def cypher_object(self):\n",
    "        return self._cypher_safe(self.object)\n",
    "\n",
    "    def cypher_verb(self):\n",
    "        return self._cypher_safe(self.verb)\n",
    "\n",
    "    def _cypher_safe(self, token):\n",
    "        if token is None:\n",
    "            return \"\"\n",
    "        if type(token) is list: \n",
    "            text = ''.join([t.text_with_ws for t in token])\n",
    "        else:\n",
    "            text = token.text\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text.replace(\"'\", \"\")\n",
    "\n",
    "class TitleProcessor:\n",
    "    \n",
    "    def _to_nltk_tree(self, node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(node.orth_, [self._to_nltk_tree(child) for child in node.children])\n",
    "        else:\n",
    "            return node.orth_\n",
    "        \n",
    "    def _debug_token(self, token):\n",
    "        print(f\"text: {token.text}\")\n",
    "        print(f\"dep: {token.dep_}\")\n",
    "        print(f\"head dep: {token.head.dep_}\")\n",
    "        print(f\"head head pos: {token.head.head.pos_}\")\n",
    "        print(f\"lefts: {list(token.lefts)}\")\n",
    "        print()\n",
    "\n",
    "        \n",
    "class SVOProcessor(TitleProcessor):\n",
    "    \n",
    "    def process(self, doc, debug=False):\n",
    "        if debug:\n",
    "            [self._to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "        triples = []\n",
    "        for token in doc:\n",
    "            # If statements can be highly misleading (as a source of Truth)\n",
    "            if token.text == \"if\":\n",
    "                return []\n",
    "            if debug:\n",
    "                self._debug_token(token)\n",
    "            subject_object_triples = self._find_triples(token, debug)\n",
    "            if subject_object_triples:\n",
    "                triples += subject_object_triples\n",
    "        return triples\n",
    "\n",
    "    def _find_triples(self, token, debug=False):\n",
    "        is_object_of_prepositional_phrase = self._is_object_of_prepositional_phrase(token)\n",
    "        if is_object_of_prepositional_phrase:\n",
    "            if debug:\n",
    "                print(\"is_object_of_prepositional_phrase\")\n",
    "            return is_object_of_prepositional_phrase\n",
    "        is_object = self._is_object(token)\n",
    "        if is_object:\n",
    "            if debug:\n",
    "                print(\"is_object\")\n",
    "            return is_object\n",
    "    \n",
    "    def _verbs(self):\n",
    "        return [\"VERB\", \"AUX\"]    \n",
    "    \n",
    "    def _is_object_of_prepositional_phrase(self, token):\n",
    "        # Finds objects of prepositional phrases\n",
    "        # eg \"Apply online for a UK passport\", \"Apply for this licence\"\n",
    "        if (token.dep_ == \"pobj\" and token.head.dep_ == \"prep\") or (token.dep_==\"dobj\" and token.head.dep_ == \"xcomp\") and token.head.head.pos_ in self._verbs():\n",
    "            print(f\"AHA ITS A PREP PHRASE, token is : {token.text}\")\n",
    "            triple = SOV()\n",
    "            triple.verb = token.head.head\n",
    "            triple.object = [token]\n",
    "            # experiment\n",
    "            triple.subject = []\n",
    "            reversed_lefts = list(token.lefts) or []\n",
    "            reversed_lefts.reverse()# or []\n",
    "            print(f\"reversed lefts are: {reversed_lefts}\")\n",
    "            if reversed_lefts:\n",
    "                for left in reversed_lefts:\n",
    "                    print(f\"left text: {left.text}\")\n",
    "                    print(f\"left dep: {left.dep_}\")\n",
    "                    if left.dep_ == \"poss\":\n",
    "                        triple.subject.append(left)\n",
    "                        print(f\"After appending lefts, subject is now: {triple.subject}\")\n",
    "            # end experiment\n",
    "            compound_lefts = self._compound_left_compounds(token)\n",
    "            if any(compound_lefts):\n",
    "                compound_lefts.reverse()\n",
    "                print(compound_lefts)\n",
    "                triple.object = compound_lefts + triple.object\n",
    "            return [triple]\n",
    "\n",
    "    def _is_object(self, token):\n",
    "        # Finds simple objects\n",
    "        # eg \"Get a passport for your child\"\n",
    "        # TODO: should probably extract \"for your child\" bit as a modifier of some kind\n",
    "        if token.dep_ == \"dobj\" and token.head.pos_ in self._verbs():\n",
    "            triple = SOV()\n",
    "            triple.verb = token.head.head\n",
    "            triple.object = [token]\n",
    "            compound_lefts = self._compound_left_compounds(token)\n",
    "            if any(compound_lefts):\n",
    "                compound_lefts.reverse()\n",
    "                print(f\"reversed compound lefts are: {compound_lefts}\")\n",
    "                triple.object = compound_lefts + triple.object\n",
    "                print(f\"object is now: {triple.object}\")\n",
    "            return [triple]\n",
    "\n",
    "    def _compound_left_compounds(self, token):\n",
    "        print(f\"compounded lefts for token: {token.text}\")\n",
    "        compounded_lefts = []\n",
    "        reversed_lefts = list(token.lefts) or []\n",
    "        reversed_lefts.reverse()# or []\n",
    "        print(reversed_lefts)\n",
    "        if reversed_lefts:\n",
    "            for left in reversed_lefts:\n",
    "                print(f\"left text: {left.text}\")\n",
    "                print(f\"left dep: {left.dep_}\")\n",
    "                if left.dep_ == \"compound\":\n",
    "                    compounded_lefts.append(left)\n",
    "                    compounded_lefts += self._compound_left_compounds(left)\n",
    "                else:\n",
    "                    break\n",
    "        return compounded_lefts\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "class EntityCombinationProcesor(TitleProcessor):\n",
    "    # This isn't used at the moment as it's low impact but worth keeping\n",
    "    # The basic idea is that there are plenty of content items whose title\n",
    "    # is something like \"Tourette's syndrome and driving\" where the \"and\" combination\n",
    "    # is important - as it indicates that it's specifically related to Tourette's \n",
    "    # syndrome and it's relation to driving\n",
    "    #\n",
    "    # Known texts that perform well\n",
    "    # Optic neuritis and driving\n",
    "    # Tourette's syndrome and driving\n",
    "    # \n",
    "    # Texts that don't work/need further work\n",
    "    # Kindertransport and the State Pension\n",
    "    # Stroke (cerebrovascular accident) and driving\n",
    "    \n",
    "    \n",
    "    def process(self, doc, debug = False):\n",
    "        if debug:\n",
    "            [self._to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "        triples = []\n",
    "        last_cc_token = None\n",
    "        objects = []\n",
    "        join = None\n",
    "        for token in doc:\n",
    "            if debug:\n",
    "                self._debug_token(token)\n",
    "            if token.dep_ == \"cc\":\n",
    "                last_cc_token = token\n",
    "            if token.dep_ == \"conj\" and last_cc_token:\n",
    "                head_token = compound_left_compounds(token.head) + [token.head]\n",
    "                print(f\"after adding left compounds, token is now: {head_token}\")\n",
    "                objects.append(head_token)\n",
    "                objects.append(token)\n",
    "                join = last_cc_token\n",
    "        if any(objects):\n",
    "            return [objects, join]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def _compound_left_compounds(self, token):\n",
    "            compounded_lefts = []\n",
    "            reversed_lefts = list(token.lefts) or []\n",
    "            reversed_lefts.reverse()# or []\n",
    "            print(f\"compounded lefts for token: {token.text} are {reversed_lefts}\")\n",
    "            if reversed_lefts:\n",
    "                for left in reversed_lefts:\n",
    "                    print(f\"left text: {left.text}\")\n",
    "                    print(f\"left dep: {left.dep_}\")\n",
    "                    if left.dep_ == \"amod\":\n",
    "                        compounded_lefts.append(left)\n",
    "                        compounded_lefts += compound_left_compounds(left)\n",
    "                    else:\n",
    "                        break\n",
    "            return compounded_lefts\n",
    "\n",
    "class PageTitleExtractor:\n",
    "    def __init__(self, page, nlp):\n",
    "        self.page = page\n",
    "        self.nlp = nlp\n",
    "        self.titles = []\n",
    "        \n",
    "    def extract(self):\n",
    "        if any(self.titles):\n",
    "            return self.titles\n",
    "        if os.path.exists(self.page.html_file_path()):\n",
    "            # I have an old copy of the mirrors so sometimes the file won't exist\n",
    "            with open(self.page.html_file_path(), \"r\") as html_file:\n",
    "                html = html_file.read()\n",
    "                soup = BeautifulSoup(html)\n",
    "                soup = self._get_main_content(soup)\n",
    "                # TODO: Sometimes soup is None, not sure why, should investigate\n",
    "                # I _suspect_ that it's that the page doesn't have a \"main\" tag\n",
    "                if soup:\n",
    "                    titles = self._extract_titles(soup)\n",
    "                    for title in titles:\n",
    "                        title_instance = Title(title, self.nlp)\n",
    "                        self.titles.append(title_instance)\n",
    "        return self.titles\n",
    "\n",
    "    \n",
    "    def _get_main_content(self, soup):\n",
    "        # Sometimes nav. items are in the main content block\n",
    "        # so we need to remove those too\n",
    "        regex = re.compile('nav')\n",
    "        for nav_item in soup.find_all(class_=regex):\n",
    "                nav_item.decompose()\n",
    "        regex = re.compile('feedback')\n",
    "        for nav_item in soup.find_all(class_=regex):\n",
    "                nav_item.decompose()\n",
    "        for nav_item in soup.find_all(\"nav\"):\n",
    "                nav_item.decompose()\n",
    "        return soup.find(\"main\")\n",
    "\n",
    "    def _extract_titles(self, soup):\n",
    "        for tag in ['b', 'i', 'u', 'a', 'abbr']:\n",
    "            for match in soup.findAll(tag):\n",
    "                match.replaceWithChildren()\n",
    "                # If we don't extract them, the old tags stick\n",
    "                # around and mess up the soup.strings call\n",
    "        # This extracts all headers, commented out as at the moment we're only doing the title\n",
    "#         headers = soup.find_all(re.compile('^h[1-6]$'))\n",
    "        headers = soup.find_all(re.compile('^h1$'))\n",
    "        titles = []\n",
    "        for header in headers:\n",
    "            strings = [re.sub(' +', ' ', string) for string in list(header.strings)]\n",
    "            strings = [string.replace(\"\\n\", \"\") for string in strings]\n",
    "            strings = [string.strip() for string in strings]\n",
    "            titles += strings\n",
    "        return titles\n",
    "            \n",
    "class Title:\n",
    "    def __init__(self, title, nlp):\n",
    "        self.title = title\n",
    "        self.doc = nlp(title)\n",
    "        self.triples = []\n",
    "        self.computed_triples = False\n",
    "        self.combinations = []\n",
    "        self.computed_combinations = False\n",
    "        \n",
    "    def subject_object_triples(self, debug=False):\n",
    "        if self.computed_triples:\n",
    "            return self.triples\n",
    "        print(f\"debug at title: {debug}\")\n",
    "        self.triples = SVOProcessor().process(self.doc, debug)\n",
    "        self.computed_triples = True\n",
    "        return self.triples\n",
    "    \n",
    "    def entity_combinations(self):\n",
    "        if self.computed_combinations:\n",
    "            return self.combinations\n",
    "        self.combinations = EntityCombinationProcesor().process(self.doc)\n",
    "        self.computed_combinations = True\n",
    "        return self.combinations\n",
    "        \n",
    "class Page:\n",
    "    def __init__(self, content_item, nlp, html_content_dir_path):\n",
    "        self.content_item = content_item\n",
    "        self.nlp = nlp\n",
    "        self.html_content_dir_path = html_content_dir_path\n",
    "        self.extracted_titles = []\n",
    "        \n",
    "    def base_path(self):\n",
    "        return self.content_item['base_path']\n",
    "    \n",
    "    def html_file_path(self):\n",
    "        return f\"{self.html_content_dir_path}{self.base_path()}.html\"\n",
    "    \n",
    "    def titles(self):\n",
    "        if any(self.extracted_titles):\n",
    "            return self.extracted_titles\n",
    "        self.extracted_titles = PageTitleExtractor(self, self.nlp).extract()\n",
    "        return self.extracted_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages = []\n",
    "for index, content_item in mainstream_content.iterrows():\n",
    "    pages.append(Page(content_item, nlp, html_content_dir_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert SVO triples into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_actions(page, triple):\n",
    "    action_name = f\"{triple.cypher_verb()} {triple.cypher_object()}\"\n",
    "    if triple.subject:\n",
    "        action_name = f\"{triple.cypher_subject()} {action_name}\"\n",
    "        graph.run(\"MATCH (page:Cid { name: '\" + page.base_path() + \"'}) \\\n",
    "        MATCH (object:Entity{name: '\" + triple.cypher_object() + \"'}) \\\n",
    "        MATCH (subject:Entity{name: '\" + triple.cypher_subject() + \"'}) \\\n",
    "        MERGE (verb:Verb{name: '\" + triple.cypher_verb() + \"'}) \\\n",
    "        MERGE (action:Action{name: '\" + action_name + \"'}) \\\n",
    "        MERGE (action)<-[:TITLE_MENTIONS]-(page) \\\n",
    "        MERGE (action)-[:HAS_VERB]->(verb) \\\n",
    "        MERGE (action)-[:HAS_OBJECT]->(object) \\\n",
    "        MERGE (action)-[:HAS_SUBJECT]->(subject);\")\n",
    "    else:\n",
    "        graph.run(\"MATCH (page:Cid { name: '\" + page.base_path() + \"'}) \\\n",
    "        MATCH (object:Entity{name: '\" + triple.cypher_object() + \"'}) \\\n",
    "        MERGE (verb:Verb{name: '\" + triple.cypher_verb() + \"'}) \\\n",
    "        MERGE (action:Action{name: '\" + action_name + \"'}) \\\n",
    "        MERGE (action)<-[:TITLE_MENTIONS]-(page) \\\n",
    "        MERGE (action)-[:HAS_VERB]->(verb) \\\n",
    "        MERGE (action)-[:HAS_OBJECT]->(object);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert title actions into graph\n",
    "# Once it has run (shouldn't take long) inspect the graph to see how it works (in particular look at Action nodes and their linkages)\n",
    "\n",
    "host = os.environ.get('REMOTE_NEO4J_URL')\n",
    "user = os.environ.get('NEO4J_USER')\n",
    "password = os.environ.get('NEO4J_PASSWORD')\n",
    "graph = Graph(host=host, user='neo4j', password = password, secure=True)\n",
    "\n",
    "for page in pages:\n",
    "    if any(page.titles()):\n",
    "        title = page.titles()[0]\n",
    "        for triple in title.subject_object_triples():\n",
    "            create_actions(page, triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a specific call to find pages that allow you to \"renew licence\"\n",
    "\n",
    "graph.run('MATCH ({name: \"renew\"})-[:HAS_VERB|HAS_OBJECT|HAS_SUBJECT]-(n:Action)-[:HAS_VERB|HAS_OBJECT|HAS_SUBJECT]-({name: \"licence\"}) WITH n MATCH (n)-[:TITLE_MENTIONS]-(c:Cid) return c.name').data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at pages that don't have any triples and see if we can find some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_triples_or_combinations = []\n",
    "for page in pages:\n",
    "    if any(page.titles()):\n",
    "        if len(page.titles()[0].subject_object_triples()) == 0 and len(page.titles()[0].entity_combinations()) == 0:\n",
    "            no_triples_or_combinations.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pages))\n",
    "len(no_triples_or_combinations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all verbs and objects for content (ie what can do you do some things)\n",
    "\n",
    "This is the section to run to export data for the finder frontend prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {}\n",
    "verbs = {}\n",
    "a = \"\"\n",
    "for page in pages:\n",
    "    if any(page.titles()):\n",
    "        for title in page.titles():\n",
    "            for triple in title.subject_object_triples():\n",
    "                triple_object = triple.cypher_object()\n",
    "                triple_verb = triple.cypher_verb()\n",
    "                if triple_object not in objects:\n",
    "                    objects[triple_object] = []\n",
    "                objects[triple_object].append([triple_verb, page.base_path(), title.title])\n",
    "                if triple_verb not in verbs:\n",
    "                    verbs[triple_verb] = []\n",
    "                verbs[triple_verb].append([triple_object, page.base_path(), title.title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in objects.items():\n",
    "    unique_v = []\n",
    "    for item in v:\n",
    "        found = False\n",
    "        for unique_item in unique_v:\n",
    "            if unique_item[0] == item[0]:\n",
    "                found = True\n",
    "        if not found:\n",
    "            unique_v.append(item)\n",
    "    objects[k] = unique_v\n",
    "    \n",
    "for k, v in verbs.items():\n",
    "    unique_v = []\n",
    "    for item in v:\n",
    "        found = False\n",
    "        for unique_item in unique_v:\n",
    "            if unique_item[0] == item[0]:\n",
    "                found = True\n",
    "        if not found:\n",
    "            unique_v.append(item)\n",
    "    verbs[k] = unique_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump out results for usage in finder-frontend\n",
    "\n",
    "import json\n",
    "\n",
    "with open('objects.json', 'w') as json_file:\n",
    "    json.dump(objects, json_file)\n",
    "    \n",
    "with open('verbs.json', 'w') as json_file:\n",
    "    json.dump(verbs, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for debugging SOV extraction\n",
    "\n",
    "text = \"Appeal a Housing Benefit decision\"\n",
    "text = \"Apply for Universal Credit\"\n",
    "text = \"Housing Benefit can help you pay your rent if you’re unemployed, on a low income or claiming benefits.\"\n",
    "text = \"Get state pension\"\n",
    "text = \"Order a commemorative marriage certificate\"\n",
    "\n",
    "# ands\n",
    "# Déjà vu and driving\n",
    "# Severe memory problems and driving\n",
    "# Kindertransport and the State Pension\n",
    "# Optic neuritis and driving\n",
    "# Tourette's syndrome and driving\n",
    "# Stroke (cerebrovascular accident) and driving\n",
    "\n",
    "# Vehicle recalls and faults\n",
    "# VAT visits and inspections\n",
    "# Farm and livery horses\n",
    "# Maternity pay and leave\n",
    "\n",
    "# Tattoo, piercing and electrolysis licence (Northern Ireland)\n",
    "# Your benefits, tax and pension after the death of a spouse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ors\n",
    "# Laser or intense pulsed light treatment licence (Northern Ireland)\n",
    "# Brain abscess, cyst or encephalitis and driving\n",
    "# Dizziness or vertigo and driving\n",
    "\n",
    "# # fors\n",
    "# Planning permission for farms\n",
    "# Claim tax relief for your job expenses\n",
    "# Master certificate for forest reproductive material (Northern Ireland)\n",
    "# Driver CPC training for qualified drivers\n",
    "# Tax relief for community amateur sports clubs (CASCs)\n",
    "# Support for Mortgage Interest (SMI)\n",
    "# Compensation for victims of modern slavery and human trafficking\n",
    "# Reduced rate National Insurance for married women\n",
    "# PAYE and payroll for employers\n",
    "# Subsidised transport for 16 to 19 year olds in education\n",
    "# Vehicle access permit for pedestrian zones (Northern Ireland)\n",
    "\n",
    "# # other interesting\n",
    "# Statutory Sick Pay (SSP): employer guide\n",
    "# Student finance: how to apply\n",
    "# VAT on services from abroad\n",
    "# Licence for projections over a highway (England & Wales)\n",
    "# Haulage jobs in the EU\n",
    "# Rights and responsibilities for reservists and employers\n",
    "# Budget for your Self Assessment tax bill if you're self-employed\n",
    "# Tax on your UK income if you live abroad\n",
    "# Dispose of waste in sea (Scotland)\n",
    "# Safety certificates: sports grounds (England, Scotland and Wales)\n",
    "\n",
    "# # SVO\n",
    "# Exchange your paper driving licence for a photocard licence\n",
    "# Check if a vehicle is taxed\n",
    "# Stop being an employer\n",
    "# Check if an animal medicine is licensed\n",
    "# Request CCTV footage of yourself\n",
    "# Volunteer as a coastguard\n",
    "# Get your court costs assessed and approved\n",
    "# Book internet access in your library\n",
    "# Claim asylum in the UK\n",
    "# Check if an alcohol wholesaler is approved\n",
    "# Check if an awarding body is recognised\n",
    "# Check if a university or college is officially recognised\n",
    "# Become a motorcycle instructor\n",
    "# Become a magistrate\n",
    "# Comment on an alcohol licence\n",
    "# Become an approved building inspector (England and Wales)\n",
    "\n",
    "text = \"Severe memory problems and driving\"\n",
    "# Kindertransport and the State Pension\n",
    "# Optic neuritis and driving\n",
    "# Tourette's syndrome and driving\n",
    "# Stroke (cerebrovascular accident) and driving\n",
    "\n",
    "text = \"Apply to adopt a child through your council\"\n",
    "t = Title(text, nlp)\n",
    "sov = t.subject_object_triples(debug=True)[1]\n",
    "print(f\"subject: {sov.subject}\")\n",
    "print(f\"object: {sov.object}\")\n",
    "print(f\"verb: {sov.verb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    \n",
    "text = \"Optic neuritis and driving\"\n",
    "text = \"Kindertransport and the State Pension\"\n",
    "# Optic neuritis and driving\n",
    "# Tourette's syndrome and driving\n",
    "# \n",
    "text = \"Stroke (cerebrovascular accident) and driving\"\n",
    "EntityCombination().process(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framenet\n",
    "\n",
    "Offers a really good framework to 'cluster' actions that are possible and help synonymise verbs, for example in many cases \"get\" and \"apply\" for something are effectively synonyms and are used interchanably. Linking them through frames will help us answer queries that don't use the exact same verb as the one in the content title (as well as offering ways to find all content that (say) is about \"applying for something\"\n",
    "\n",
    "It's a little more complex than I'd initially thought so I'm keeping this here for future use as I think it will be useful but it was a bit of trying to run before we could walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting FrameNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from py2neo import Graph\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some inheritance munging\n",
    "\n",
    "frames_data = []\n",
    "for frame in fn.frames():\n",
    "    lexeme_units = list(frame['lexUnit'].keys())\n",
    "    fe = list(frame[\"FE\"].keys())\n",
    "    for relation in frame['frameRelations']:\n",
    "        if relation['type']['name'] == \"Inheritance\" and relation['Child'] == frame:\n",
    "            # If it's an inheritance and the child is the frame in question\n",
    "            lexeme_units += list(relation['Parent']['lexUnit'].keys())\n",
    "            fe += list(relation['Parent']['FE'].keys())\n",
    "    frame_data = {\n",
    "        'name': frame['name'],\n",
    "        'lexeme_units': lexeme_units,\n",
    "        'fe': fe\n",
    "    }\n",
    "    frames_data.append(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKES A LOOOOOONG TIME\n",
    "\n",
    "host = os.environ.get('REMOTE_NEO4J_URL')\n",
    "user = os.environ.get('NEO4J_USER')\n",
    "password = os.environ.get('NEO4J_PASSWORD')\n",
    "graph = Graph(host=host, user='neo4j', password = password, secure=True)\n",
    "\n",
    "cypher = \"\"\n",
    "\n",
    "for index, frame in enumerate(frames_data[447 + 529:]):\n",
    "    print(f\"{index} of {len(frames_data)}\")\n",
    "    graph.run(\"CREATE (f:Frame {name: '\" + frame['name'] + \"'})\")\n",
    "    for lexeme_unit in list(set(frame['lexeme_units'])):\n",
    "        split_lexeme = lexeme_unit.split(\".\")\n",
    "        if split_lexeme[1] == \"v\":\n",
    "            word = split_lexeme[0].replace(\"'\", \"\")\n",
    "            graph.run(\"MATCH (f:Frame {name: '\" + frame['name'] + \"'}) \\\n",
    "            MERGE (w:Verb { word: '\" + word + \"'}) \\\n",
    "            MERGE (w)<-[:HAS_LEXEME_UNIT]-(f)\")\n",
    "    for fe in frame['fe']:\n",
    "        fe = fe.replace(\"'\", \"\")\n",
    "        fe = fe.lower()\n",
    "        if len(graph.run(\"MATCH (e:Entity {name: '\" + fe + \"'}) RETURN e\").data()) > 0:\n",
    "            graph.run(\"MATCH (f:Frame { name: '\" + frame['name'] + \"'}), \\\n",
    "                (e:Entity { name: '\" + fe + \"' }) \\\n",
    "                SET e:FrameElement \\\n",
    "                MERGE (e)<-[:HAS_FRAME_ELEMENT]-(f)\")\n",
    "        else:\n",
    "            graph.run(\"MATCH (f:Frame {name: '\" + frame['name'] + \"'}) \\\n",
    "            MERGE (w:FrameElement { type: '\" + fe + \"'})<-[:HAS_FRAME_ELEMENT]-(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using framenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frames_with_frame_element(triple):\n",
    "    frame_element_results = graph.run(\"\\\n",
    "        MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'})-[]-(frame: Frame)-[]-(fe:FrameElement { name: '\" + triple.cypher_object() + \"'})\\\n",
    "        RETURN distinct(frame.name) as frame_name\").data()\n",
    "    if not any(frame_element_results) and len(triple.cypher_subject()) > 0:\n",
    "        frame_element_results = graph.run(\"\\\n",
    "         MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'})-[]-(frame: Frame)-[]-(fe:FrameElement) \\\n",
    "         WHERE lower(fe.name) CONTAINS 'entity' \\\n",
    "         RETURN distinct(frame.name) as frame_name\").data()\n",
    "    return [frame_name['frame_name'] for frame_name in frame_element_results]\n",
    "\n",
    "def frames_without_frame_element(triple):\n",
    "    frame_results = graph.run(\"\\\n",
    "        MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'})-[]-(frame: Frame)-[]-(fe:FrameElement) \\\n",
    "        RETURN distinct(frame.name) as frame_name\")\n",
    "    return [frame_name['frame_name'] for frame_name in frame_results]\n",
    "\n",
    "def frames(triple):\n",
    "    frames = frames_with_frame_element(triple)\n",
    "    if any(frames):\n",
    "        print(\"found frame element\")\n",
    "        return frames\n",
    "    frames = frames_without_frame_element(triple)\n",
    "    return frames\n",
    "\n",
    "    \n",
    "def create_links(page, triple):\n",
    "    frames = frames_with_frame_element(triple)\n",
    "    if any(frames):\n",
    "        for frame in frames:\n",
    "            if triple.subject:\n",
    "                print(f\"frame element for frame with subject: {frame}\")\n",
    "                graph.run(\"MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'}), \\\n",
    "                (page:Cid { name: '\" + page.base_path() + \"'}), \\\n",
    "                (frame: Frame {name: '\" + frame + \"'}), \\\n",
    "                (frameElement {name: '\" + triple.cypher_object() + \"'}), \\\n",
    "                (subject: Entity {name: '\" + triple.cypher_subject() + \"'}) \\\n",
    "                CREATE (createdFrame:PageFrame { name: '\" + frame + \"'}) \\\n",
    "                CREATE (createdFrame)<-[:IS_INSTANCE_OF]-(frame) \\\n",
    "                CREATE (verb)<-[:HAS_VERB]-(page) \\\n",
    "                CREATE (frameElement)<-[:HAS_FRAME_ELEMENT]-(page) \\\n",
    "                CREATE (verb)<-[:HAS_VERB]-(createdFrame) \\\n",
    "                CREATE (createdFrame)<-[:HAS_FRAME]-(page)\\\n",
    "                CREATE (frameElement)<-[:HAS_OBJECT]-(createdFrame)\\\n",
    "                CREATE (createdFrame)-[:SUBJECT]->(subject)\")\n",
    "            else:\n",
    "                print(f\"frame element for frame without subject: {frame}\")\n",
    "                graph.run(\"MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'}), \\\n",
    "                (page:Cid { name: '\" + page.base_path() + \"'}), \\\n",
    "                (frame: Frame {name: '\" + frame + \"'}), \\\n",
    "                (frameElement {name: '\" + triple.cypher_object() + \"'}) \\\n",
    "                CREATE (createdFrame:PageFrame { name: '\" + frame + \"'}) \\\n",
    "                CREATE (createdFrame)<-[:IS_INSTANCE_OF]-(frame) \\\n",
    "                CREATE (verb)<-[:HAS_VERB]-(page) \\\n",
    "                CREATE (frameElement)<-[:HAS_FRAME_ELEMENT]-(page) \\\n",
    "                CREATE (verb)<-[:HAS_VERB]-(createdFrame) \\\n",
    "                CREATE (createdFrame)<-[:HAS_FRAME]-(page)\\\n",
    "                CREATE (frameElement)<-[:HAS_FRAME_ELEMENT]-(createdFrame)\")\n",
    "    else:\n",
    "        print(\"no frame lements\")\n",
    "        frames = frames_without_frame_element(triple)\n",
    "        for frame in frames:\n",
    "            graph.run(\"MATCH (verb:Verb{word: '\" + triple.cypher_verb() + \"'}), \\\n",
    "            (page:Cid { name: '\" + page.base_path() + \"'}), \\\n",
    "            (frame: Frame {name: '\" + frame + \"'}) \\\n",
    "            CREATE (createdFrame:PageFrame { name: '\" + frame + \"'}) \\\n",
    "            CREATE (createdFrame)<-[:IS_INSTANCE_OF]-(frame) \\\n",
    "            CREATE (verb)<-[:HAS_VERB]-(page) \\\n",
    "            CREATE (verb)<-[:HAS_VERB]-(createdFrame) \\\n",
    "            CREATE (createdFrame)<-[:HAS_FRAME]-(page)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link pages to frames, once it has run, have a poke around some PageFrame objects to see how it works\n",
    "\n",
    "host = os.environ.get('REMOTE_NEO4J_URL')\n",
    "user = os.environ.get('NEO4J_USER')\n",
    "password = os.environ.get('NEO4J_PASSWORD')\n",
    "graph = Graph(host=host, user='neo4j', password = password, secure=True)\n",
    "\n",
    "for page in pages:\n",
    "    for title in page.titles():\n",
    "        for triple in title.subject_object_triples():\n",
    "            create_links(page, triple)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
